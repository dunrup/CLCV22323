{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a42be27-4447-46a6-b279-d92c8d4402b7",
   "metadata": {},
   "source": [
    "# Latin Text Prep Aid - Vocabulary Builder\n",
    "This program will take in a Latin and text and return an excel file with multiple tabs of useful vocabulary lists to aid in preparing texts to be taught. The tabs will be:\n",
    " 1. Full parsed text, which will give each word from the text as a line of data with its lemma, frequency, definition, etc.\n",
    " 2. A 'priority vocab' list, which will give each word that does *not* appear in the [DCC Latin Core vocabulary](https://dcc.dickinson.edu/vocab/core-vocabulary) (top 1000 frequency list)\n",
    " 3. The tagged core list, to compare\n",
    "\n",
    "In order to run this notebook, ensure that you have installed/downloaded:\n",
    "- [pandas](https://pandas.pydata.org/)\n",
    "- [spaCy](https://spacy.io/)\n",
    "- [latinCy](https://diyclassics.github.io/latincy/)\n",
    "\n",
    "In addition, download the 'data' folder and save it in the working directory with this notebook. It includes:\n",
    "- lat_core.xlsx, a modified Latin core list based off [DCC Latin Core vocabulary](https://dcc.dickinson.edu/vocab/core-vocabulary)\n",
    "- 'lat_lewis_elementary_lexicon', a modified Lewis & Short dictionary used to get definitions, from [Perseus Project](https://www.perseus.tufts.edu/hopper/opensource/download).\n",
    "- a few texts from downloaded from [the Latin Library](https://thelatinlibrary.com/)\n",
    "\n",
    "To run this program on your own text, rather than the livy sample used here, put your Latin text as a txt file in the data folder and change the appropriate variable in *1. Text selection* below.\n",
    "\n",
    "NOTE: Works on **Ubuntu**. Doesn't work on Windows. Don't know why. (The pandas merge function throws errors on Windows.)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933b5801-6ffd-4381-b1df-4caaedbc7cfc",
   "metadata": {},
   "source": [
    "## 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80931fc8-e6c9-49f9-8536-232ee9757bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the following to run the program:\n",
    "\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from sklearn.manifold import TSNE\n",
    "from spacy import displacy\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c23826-cad5-469e-9459-dafda19fb222",
   "metadata": {},
   "source": [
    "## 1. Text selection \n",
    "Put any latin text you'd like in the folder 'data,' which lives in the folder with this notebook. Replace *'lat-livy.txt'* with your text of choice.\n",
    "\n",
    "\n",
    "The process below will print the first line of your text, to verify it's what you've put in, and then the length by characters and by words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8599357c-df73-44aa-aa6f-7894ae08593e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First line: Iam primum omnium satis constat Troia capta in ceteros saeuitum esse Troianos, duobus, Aeneae Anteno\n",
      "Character count: 920884\n",
      "Approximate token count total: 130066\n"
     ]
    }
   ],
   "source": [
    "# Set your input. Replace *lat-livy.txt* with your text file. You can uncomment the caesar or vergil for testing.\n",
    "working_directory = os.getcwd()\n",
    "# file_path = working_directory + '/data/lat_text_latin_library/caesar/gall1.txt'\n",
    "# file_path = working_directory + '/data/lat_text_latin_library/vergil/aen1.txt'\n",
    "file_path = working_directory + '/data/lat-livy.txt'\n",
    "\n",
    "# read in your text\n",
    "with open(file_path) as f:\n",
    "    text_full = f.read()\n",
    "\n",
    "# \"replace\" command gets rid of line breaks, changes v's -> u's, and em-dashes to spaces\n",
    "text_full = text_full.replace(\"\\n\", \" \").replace(\"\\t\", \" \").replace(\"v\",\"u\").replace(\"V\",\"U\").replace(\"â€”\",\" \")\n",
    "\n",
    "# Check that this is the correct file, and see the length\n",
    "print(f\"First line: {text_full[:100]}\")\n",
    "print(f\"Character count: {len(text_full)}\")\n",
    "print(f\"Approximate token count total: {len(text_full.split())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4ec485-fb84-4cf1-9564-cafa20ff7f59",
   "metadata": {},
   "source": [
    "### text length problems\n",
    "On my little old laptop, any text above ~20,000 words crashes Jupyter Notebook. Solving this problem (via multithreading, or increasing memory allowed to Jupyter, or something) is not currently a priority. I will fix this someday, but not today.\n",
    "\n",
    "So, the process below splits the text down to ~20,000 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9676a486-9f3a-4067-90bd-3d285272b3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate token count after cutting down: 21890\n"
     ]
    }
   ],
   "source": [
    "# shorting for example purposes (and for speed) to a doc of ~20,000 tokens\n",
    "if len(text_full.split()) < 20000:\n",
    "    text=text_full\n",
    "    print(f\"your text is short already\")\n",
    "else:\n",
    "    divisor = (len(text_full.split())) // 20000\n",
    "    text = text_full[:len(text_full) // divisor]\n",
    "    print(f\"Approximate token count after cutting down: {len(text.split())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88cfbf6-5bdc-4db8-8488-dc87c6469508",
   "metadata": {},
   "source": [
    "## 2. Run NLP on the text, validate things look right, and convert to a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26824ab9-7e5d-4e6c-a653-558ca081ddac",
   "metadata": {},
   "source": [
    "### run NLP and validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "377a1a7c-34bc-4aca-b2f7-7cb1377dc5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up spaCy NLP\n",
    "model = 'la_core_web_lg'\n",
    "nlp = spacy.load(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba58968c-7a88-40a9-9e12-af754daa2da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spacy Doc object - this might take a minute\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "222b990c-9ca3-41f4-b025-6b6d01babbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (FOR DEBUGGING, UNNECCESSARY FOR THE FINAL PRODUCT)\n",
    "# prints the text first's word\n",
    "# print(doc[0])\n",
    "# print(type(doc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58f5795b-b29e-4672-aa7d-2ec6f0074ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # (FOR DEBUGGING, UNNECCESSARY FOR THE FINAL PRODUCT)\n",
    "# # All the attributes LatinCy has tagged for the first word\n",
    "# print([item for item in dir(doc[0]) if not item.startswith(\"_\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0837ada-b4a3-4ba7-b507-228a48f6d9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (FOR DEBUGGING, UNNECCESSARY FOR THE FINAL PRODUCT)\n",
    "# Run this to get sentences from text to verify parsing \n",
    "# sents = doc.sents\n",
    "# for i, x in enumerate(sents, 1):\n",
    "#         print(f\"{i}: {x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b36936b-c2be-4acc-a99a-4826aa3a58d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (FOR DEBUGGING, UNNECCESSARY FOR THE FINAL PRODUCT)\n",
    "# number = 0\n",
    "# for token in doc[:]:\n",
    "#     if token.text == ' ':\n",
    "#         number = number+1\n",
    "#         print(token.lemma_)\n",
    "\n",
    "# number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1835d595-3fcd-4676-9f37-85526cb98d86",
   "metadata": {},
   "source": [
    "### make dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65299912-267d-469a-8b01-f8a47b0c50c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dataframe with token attributes. first, make a list for all the tokens\n",
    "data = []\n",
    "for token in doc[:]:\n",
    "    if token.text != ' ': \n",
    "        data.append(\n",
    "            [\n",
    "                token.text,\n",
    "                token.norm_,\n",
    "                token.lower_,\n",
    "                token.lemma_,\n",
    "                token.pos_,\n",
    "                token.tag_,\n",
    "                token.text in nlp.vocab,\n",
    "                \n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a273e502-d8b0-4af3-bffa-bf378f4270c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# then make that [] into a df with the proper columns\n",
    "df = pd.DataFrame(\n",
    "    data,\n",
    "    columns=[\n",
    "        \"text\",\n",
    "        \"norm\",\n",
    "        \"lower\",\n",
    "        \"lemma\",\n",
    "        \"pos\",\n",
    "        \"tag\",\n",
    "        \"in_vocab\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6841d0f9-07a8-42e6-bfe6-e20f69a3bb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (FOR DEBUGGING, UNNECCESSARY FOR THE FINAL PRODUCT)\n",
    "# info about the first word\n",
    "# print(f' text: {doc[0].text}, norm: {doc[0].norm_}, lemma: {doc[0].lemma_}, POS: {doc[0].pos_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be457dab-c503-4139-a33a-de58f71fa3b2",
   "metadata": {},
   "source": [
    "### clean the dataframe: get rid of punctuation, nulls, common errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "067e7f76-9029-496c-8552-463419857c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: 26045\n",
      "after: 25821\n"
     ]
    }
   ],
   "source": [
    "# I don't care about punctuation, so remove all the rows for punctuation\n",
    "print(f\"before: {df['text'].count()}\")\n",
    "df = df[(df['tag'] != \"number\")]\n",
    "print(f\"after: {df['text'].count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eec6e755-d3de-444d-a83d-11b8d7a9371e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: 25821\n",
      "after: 22250\n"
     ]
    }
   ],
   "source": [
    "# I don't care about numbers, so remove all the rows for numbers\n",
    "print(f\"before: {df['text'].count()}\")\n",
    "df = df[(df['tag'] != \"punc\")]\n",
    "print(f\"after: {df['text'].count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3df1e259-63bf-400f-8f5d-425dc2a97412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: 22250\n",
      "after: 22236\n"
     ]
    }
   ],
   "source": [
    "# blank space will come through as empty or null entries, or some seemingly random amount of whitespace.\n",
    "# I do not know why it comes through as seemingly random amount of whitespace.\n",
    "\n",
    "# note: I don't know why these cells don't count as null. No version of \"dropna()\" gets rid of them\n",
    "\n",
    "print(f\"before: {df['text'].count()}\")\n",
    "df = df[df['text'] != ' ']\n",
    "df = df[df['text'] != '  ']\n",
    "df = df[df['text'] != '   ']\n",
    "df = df[df['text'] != '    ']\n",
    "df = df[df['text'] != '     ']\n",
    "df = df[df['text'] != '      ']\n",
    "df = df[df['text'] != '       ']\n",
    "df = df[df['text'] != '        ']\n",
    "df = df[df['text'] != '         ']\n",
    "df = df[df['text'] != '          ']\n",
    "df = df[df['text'] != '           ']\n",
    "df = df[df['text'] != '            ']\n",
    "df = df[df['text'] != '             ']\n",
    "df = df[df['text'] != '              ']\n",
    "df = df[df['text'] != '               ']\n",
    "df = df[df['text'] != '                ']\n",
    "df = df[df['text'] != '                 ']\n",
    "df = df[df['text'] != '                  ']\n",
    "df = df[df['text'] != '                   ']\n",
    "df = df[df['text'] != '                    ']\n",
    "df = df[df['text'] != '                     ']\n",
    "df = df[df['text'] != '                      ']\n",
    "df = df[df['text'] != '                       ']\n",
    "df = df[df['text'] != '                        ']\n",
    "df = df[df['text'] != '                         ']\n",
    "df = df[df['text'] != '                          ']\n",
    "df = df[df['text'] != '                           ']\n",
    "df = df[df['text'] != '                            ']\n",
    "df = df[df['text'] != '                             ']\n",
    "df = df[df['text'] != '                              ']\n",
    "df = df[df['text'] != '                               ']\n",
    "df = df[df['text'] != '                                ']\n",
    "df = df[df['text'] != '                                 ']\n",
    "df = df[df['text'] != '                                  ']\n",
    "df = df[df['text'] != '                                   ']\n",
    "df = df[df['text'] != '                                    ']\n",
    "df = df[df['text'] != '                                     ']\n",
    "df = df[df['text'] != '                                      ']\n",
    "df = df[df['text'] != '                                       ']\n",
    "print(f\"after: {df['text'].count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b72e4c7-ac6d-451c-86b1-4dc137cfeb12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after: 22236\n",
      "after: 22236\n"
     ]
    }
   ],
   "source": [
    "# this process checks if there were any unknown words, because these will not have been parsed correctly.\n",
    "# Unknown words are removed from the frequency / comparative analyses, and exported as their own sheet in the excel\n",
    "print(f\"after: {df['text'].count()}\")\n",
    "df_unknownwords = df[(df['in_vocab'] != True)]\n",
    "df_unknownwords\n",
    "df = df[(df['in_vocab'] != False)]\n",
    "print(f\"after: {df['text'].count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4df05e8a-a556-49d4-81e1-aa83579039c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The parser makes some errors detecting lemmata. This fixes many of them.\n",
    "# I'm sure it's gonna make a lot more than this, but these are what I've found.\n",
    "df.loc[df['lemma']=='ab', 'lemma'] = \"a\"\n",
    "df.loc[df['lemma']=='eum', 'lemma'] = \"is\"\n",
    "df.loc[df['lemma']=='eam', 'lemma'] = \"is\"\n",
    "df.loc[df['lemma']=='(h)asta', 'lemma'] = \"hasta\"\n",
    "df.loc[df['lemma']=='abi', 'lemma'] = \"abeo\"\n",
    "df.loc[df['lemma']=='Patres', 'lemma'] = \"pater\"\n",
    "df.loc[df['lemma']=='patr', 'lemma'] = \"pater\"\n",
    "df.loc[df['lemma']=='omnium', 'lemma'] = \"omnis\"\n",
    "df.loc[df['lemma']=='Romana', 'lemma'] = \"romanus\"\n",
    "df.loc[df['lemma']=='magne', 'lemma'] = \"magnus\"\n",
    "df.loc[df['lemma']=='sua', 'lemma'] = \"suus\"\n",
    "df.loc[df['lemma']=='ui', 'lemma'] = \"uis\"\n",
    "df.loc[df['lemma']=='Ui', 'lemma'] = \"uis\"\n",
    "df.loc[df['lemma']=='Feo', 'lemma'] = \"sum\"\n",
    "df.loc[df['lemma']=='ror', 'lemma'] = \"reor\"\n",
    "df.loc[df['lemma']=='domi', 'lemma'] = \"domus\"\n",
    "df.loc[df['lemma']=='Consules', 'lemma'] = \"consul\"\n",
    "df.loc[df['lemma']=='bonum', 'lemma'] = \"bonus\"\n",
    "df.loc[df['lemma']=='virs', 'lemma'] = \"uir\"\n",
    "df.loc[df['lemma']=='uirs', 'lemma'] = \"uir\"\n",
    "df.loc[df['lemma']=='fuerit', 'lemma'] = \"sum\"\n",
    "df.loc[df['lemma']=='primor', 'lemma'] = \"primus\"\n",
    "df.loc[df['lemma']=='uido', 'lemma'] = \"uideo\"\n",
    "df.loc[df['lemma']=='iuuenus', 'lemma'] = \"iuuenis\"\n",
    "df.loc[df['lemma']=='coepi', 'lemma'] = \"coepio\"\n",
    "df.loc[df['lemma']=='coipio', 'lemma'] = \"coepio\"\n",
    "df.loc[df['lemma']=='Iouis', 'lemma'] = \"Iuppiter\"\n",
    "df.loc[df['lemma']=='tanto', 'lemma'] = \"tantus\"\n",
    "df.loc[df['lemma']=='quique', 'lemma'] = \"quisque\"\n",
    "df.loc[df['lemma']=='auco', 'lemma'] = \"augeo\"\n",
    "df.loc[df['lemma']=='uico', 'lemma'] = \"uinco\"\n",
    "df.loc[df['lemma']=='aeneae', 'lemma'] = \"Aeneas\"\n",
    "df.loc[df['lemma']=='Aenea', 'lemma'] = \"Aeneas\"\n",
    "df.loc[df['lemma']=='Aenee', 'lemma'] = \"Aeneas\"\n",
    "df.loc[df['text']=='capta', 'lemma'] = \"capio\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2b9669-9088-4a22-b80c-498e9330c586",
   "metadata": {},
   "source": [
    "## 3. Frequency, Comparative Analyses, English Definitions\n",
    "\n",
    "- First, get most common forms in the text and most common lemmas in the text.\n",
    "  - Add those as column to the dataframe\n",
    "- After, compare those lemmas to the top 1000 most common words.\n",
    "  - This is done by reading in an excel file I've prepared, based on [DCC Latin Core vocabulary](https://dcc.dickinson.edu/vocab/core-vocabulary)\n",
    "- (Aspirational: compare to the rest of the author/corpus/etc -- not currently implemented)\n",
    "- Finally, read in Lewis and Short to get definitions. Do this last b/c it takes a while"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca30152-1f84-4062-a35f-1cb42867108f",
   "metadata": {},
   "source": [
    "### get frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "056eb604-97cb-4f49-8568-64e00eddeee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_form_frequency = df['lower'].value_counts()\n",
    "df_lemma_frequency = df['lemma'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1846f7aa-2508-4f55-831d-bb374533ea1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (FOR DEBUGGING, UNNECCESSARY FOR THE FINAL PRODUCT)\n",
    "# print(df_form_frequency.head(5))\n",
    "# print(' ')\n",
    "# print(df_lemma_frequency.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54675309-e46b-4c96-8a94-32d6a5ea6d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(df_form_frequency,\n",
    "                  left_on= 'lower',\n",
    "                  right_on = 'lower',\n",
    "                  how= 'left')\n",
    "df.rename(columns={'count':'form_frequency'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57d245ce-fc82-49da-b43f-f4db52e4ffd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (FOR DEBUGGING, UNNECCESSARY FOR THE FINAL PRODUCT)\n",
    "# df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d89ef9a2-f2a5-4d9a-bbc9-7839083a9f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(df_lemma_frequency,\n",
    "                  left_on= 'lemma',\n",
    "                  right_on = 'lemma',\n",
    "                  how= 'left')\n",
    "df.rename(columns={'count':'lemma_frequency'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25070e17-6695-4d76-956a-940ce71f5de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (FOR DEBUGGING, UNNECCESSARY FOR THE FINAL PRODUCT)\n",
    "# df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3e3088-4322-42de-ae89-f365b74b9512",
   "metadata": {},
   "source": [
    "## 3.5. get all definitions\n",
    "\n",
    "Whew, this was a doozy. The code below will:\n",
    "- Read in Lewis & Short as an xml file ([downloded From Perseus Project](https://www.perseus.tufts.edu/hopper/opensource/download)).\n",
    "- Parse your text list against the lewis and short\n",
    "- for words with more than one definition (*many words, up to four different definitions, plus more when including variable capitalization, (e.g., dis - 'wealthy' vs Dis - 'Pluto'*)), it will concat all the different L&S definitions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c33e76-f87d-478d-b9b0-0ff73cadaa76",
   "metadata": {},
   "source": [
    "### get dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "931c7532-7b47-40d8-97c3-b91ea712fced",
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree\n",
    "from yaml import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0defe186-1d55-423d-ba5f-4daebb04a894",
   "metadata": {},
   "outputs": [],
   "source": [
    "working_directory = os.getcwd()\n",
    "dictionary_file_path = working_directory + '/data/lat_lewis_elementary_lexicon/lewis_mod.xml'\n",
    "yml_file_path = working_directory + '/data/lat_lewis_elementary_lexicon/lewis.yaml'\n",
    "\n",
    "def get_root(filename):\n",
    "    parser = etree.XMLParser(load_dtd=True, no_network=False)\n",
    "    tree = etree.parse(filename, parser=parser)\n",
    "    return tree.getroot()\n",
    "\n",
    "\n",
    "def get_entries(filename):\n",
    "    root = get_root(filename)\n",
    "    lemmata = set()\n",
    "    d = {}\n",
    "    for entry in root.findall(\".//entry\"):\n",
    "        lemma = entry.get(\"key\", \"\")\n",
    "        entry_bs = BeautifulSoup(etree.tostring(entry), features=\"lxml\")\n",
    "        d[lemma] = entry_bs.text.strip()\n",
    "        lemmata.add(lemma)\n",
    "    return d\n",
    "\n",
    "\n",
    "def save_yaml(data, filename):\n",
    "    with open(filename, \"w\") as f:\n",
    "        dump(data, f)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    entries = get_entries(dictionary_file_path)\n",
    "    save_yaml(entries, \"lewis.yaml\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a6f932-8ecc-463a-8525-1a8bf92d0177",
   "metadata": {},
   "source": [
    "### get dictionary entries for our parsed text, merge on the earlier dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78af1313-d5bd-4805-ad0e-7f4e5abc965a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to get a definition from the dictionary\n",
    "# returns only the 'simple' version of the definition, set off in the xml code by a \"$.\"\n",
    "def getdefinition(lemma):\n",
    "    definition = \"\"\n",
    "    multiple_defs_testers = [\n",
    "        lemma.lower(), \n",
    "        lemma.capitalize(),\n",
    "        lemma.lower()+\"1\", \n",
    "        lemma.capitalize()+\"1\",\n",
    "        lemma.lower()+\"2\", \n",
    "        lemma.capitalize()+\"2\", \n",
    "        lemma.lower()+\"3\",\n",
    "        lemma.capitalize()+\"3\",\n",
    "        lemma.lower()+\"4\",\n",
    "        lemma.capitalize()+\"4\",\n",
    "    ]\n",
    "\n",
    "    for words in multiple_defs_testers:\n",
    "        if words in entries:\n",
    "            definition = definition + entries[words][:entries[words].find(\"$\")].replace(\"\\n\", \" \").replace(\"  \",\" \").replace(\"  \",\" \").replace(\"  \",\" \")\n",
    "    if definition == \"\":           \n",
    "        return \"error getting definition\"\n",
    "    else:\n",
    "        return definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "728c569c-265e-4ae2-9118-4847b44e60e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_defs = []\n",
    "for lemma in df['lemma']:\n",
    "    dict_defs.append(\n",
    "        [\n",
    "            lemma,\n",
    "            getdefinition(lemma)\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c90f715d-4a09-408e-bca4-8e412747d55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_defs = pd.DataFrame(\n",
    "    dict_defs,\n",
    "    columns=[\"lemma\",\"def\"],\n",
    ")\n",
    "df_defs.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff023337-2ea0-46ed-96c6-fc56d521b03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (FOR DEBUGGING, UNNECCESSARY FOR THE FINAL PRODUCT)\n",
    "# df_defs.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "69b147db-04db-4f41-953d-4a67b1469a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(df_defs,\n",
    "                  left_on= 'lemma',\n",
    "                  right_on = 'lemma',\n",
    "                  how= 'left')\n",
    "df.rename(columns={'count':'lemma_frequency'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "81324862-a440-443d-837d-264e5546c9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'def':'definition'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1bc08663-5c3f-4ed9-9fbc-cf14dcd78845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (FOR DEBUGGING, UNNECCESSARY FOR THE FINAL PRODUCT)\n",
    "# df.head(20)\n",
    "# df[(df[\"tag\"] == 'proper_noun') & (df['definition'] == 'error getting definition')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f864c2e-de00-40b8-b162-d2d42dcfee64",
   "metadata": {},
   "source": [
    "### clean up proper nouns\n",
    "Many proper names give definition errors, so this sets the 'definition' to be the lemma (i.e., the proper name)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "be36c3e6-4f29-4786-a7a9-4952744eda58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['definition'] == \"error getting definition\") & (df['pos'] == 'PROPN'), 'definition'] = df['lemma']\n",
    "df.loc[(df['definition'] == \"error getting definition\") & (df['tag']=='proper_noun'), 'definition'] = df['lemma']\n",
    "df.loc[(df['definition'] == \"error getting definition\") & (df['tag']=='proper_noun_particle'), 'definition'] = df['lemma']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f42a4cd-1f02-402b-b567-381866db0e32",
   "metadata": {},
   "source": [
    "## 3.75 compare to top used words chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aeff25ff-120b-4953-84e1-7b33f3001df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "working_directory = os.getcwd()\n",
    "core_file_path = working_directory + '/data/core_lat.xlsx'\n",
    "df_core = pd.read_excel(core_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e41a3c78-54b2-4578-9a2e-658f0b03b9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (FOR DEBUGGING, UNNECCESSARY FOR THE FINAL PRODUCT)\n",
    "# testword = 'abs'\n",
    "# df_core[(df_core['l1'] == testword) | (df_core['l2'] == testword) | (df_core['l3'] == testword) | (df_core['l4'] == testword)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2a626569-67e5-40f6-be7c-0a2c3f5b1b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't make fun of me georgios I know you could do this in one line but I don't know how\n",
    "# just enjoy your little nested if\n",
    "\n",
    "def isfrequent(lemma):\n",
    "    if lemma in df_core['l1'].values: \n",
    "        return True\n",
    "    else:\n",
    "        if lemma in df_core['l2'].values: \n",
    "            return True\n",
    "        else:\n",
    "            if lemma in df_core['l3'].values: \n",
    "                return True\n",
    "            else:\n",
    "                if lemma in df_core['l4'].values: \n",
    "                    return True\n",
    "                else:\n",
    "                    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "23e75400-f6c0-41aa-a83b-f28909b27146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to check the lemma against the core list\n",
    "checking_against_top = []\n",
    "for lemma in df['lemma']:\n",
    "    checking_against_top.append(\n",
    "        [\n",
    "            lemma,\n",
    "            isfrequent(lemma)\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "658a582c-e01e-4d23-95ec-4081e5144357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a data frame with the new 'is in the core' tag\n",
    "df_freq = pd.DataFrame(\n",
    "    checking_against_top,\n",
    "    columns=[\"lemma\",\"tops\"],\n",
    ")\n",
    "df_freq.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6efe7e03-9d59-4b12-8661-295818b9e1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (FOR DEBUGGING, UNNECCESSARY FOR THE FINAL PRODUCT)\n",
    "# df_freq.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1a2599d2-3aa1-49cd-9384-edb66361f8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(df_freq,\n",
    "                  left_on= 'lemma',\n",
    "                  right_on = 'lemma',\n",
    "                  how= 'left')\n",
    "df.rename(columns={'tops':'in_core'},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bbc303-6579-4dd7-a670-bb61645aceb7",
   "metadata": {},
   "source": [
    "## 4. Exporting\n",
    "Here, I create a few slices that will be useful and export them to a xlsx file, with which you can make your vocab lists / charts as you please"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2773d2f-b922-4dc8-8d98-85a00086419b",
   "metadata": {},
   "source": [
    "Create all the views that I want as different data frames and export them all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1377ba23-f8ac-409a-9c0e-7b8ebfde2da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e216f426-611f-4be3-8a05-124e0293e268",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['text', 'lemma', 'pos', 'tag', 'form_frequency', 'lemma_frequency', 'definition', 'in_core']]\n",
    "df_core = df_core[['Headword', 'Definition', 'Part of Speech', 'Semantic Group', 'Frequency Rank']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ec8c4520-de63-4e75-8244-7dd4b09d4700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New words to be learned immediately: words that are not in DCC top 1000, occur more than 5 times in the text, and are not names\n",
    "df_priority_list = df[(df[\"lemma_frequency\"] > 5) & ~(df['in_core']) & (df['tag'] != 'proper_noun') & (df['definition'] != 'error getting definition')]\n",
    "df_priority_list = df_priority_list[['lemma','lemma_frequency','definition']]\n",
    "df_priority_list = df_priority_list.sort_values('lemma_frequency', ascending=False).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0482573f-6c67-42d3-93fd-2ffd731edc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates an ExcelWriter class, necessary for writing multiple dfs to the same file\n",
    "working_directory = os.getcwd()\n",
    "file_path = working_directory + '/exports/vocab_builder_output.xlsx'\n",
    "xlwriter = pd.ExcelWriter(path=file_path)\n",
    "df.to_excel(xlwriter, sheet_name='full_text',index=True)\n",
    "df_priority_list.to_excel(xlwriter, sheet_name='priority_words',index=False)\n",
    "df_core.to_excel(xlwriter, sheet_name='core',index=False)\n",
    "xlwriter.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
